---
# Complete Ceph Cluster Purge and Cleanup Playbook
# 이 플레이북은 기존 Ceph 클러스터를 완전히 제거합니다
# 주의: 모든 데이터가 삭제됩니다!
#
# 사용법:
# 1. FSID가 있는 경우:
#    ansible-playbook -i hosts-scalable.yaml 00.purge-everything.yml -e fsid=<FSID>
# 2. FSID를 모르거나 강제 제거:
#    ansible-playbook -i hosts-scalable.yaml 00.purge-everything.yml -e force_purge=true

- name: Complete Ceph Cluster Purge
  hosts: all
  become: true
  gather_facts: true
  vars:
    force_purge: "{{ force_purge | default(false) }}"

  tasks:
    - name: Discover existing Ceph FSID if not provided
      when: fsid is undefined and not force_purge
      block:
        - name: Check for existing ceph.conf
          stat:
            path: /etc/ceph/ceph.conf
          register: ceph_conf_stat

        - name: Extract FSID from ceph.conf
          shell: grep fsid /etc/ceph/ceph.conf | awk '{print $3}'
          register: discovered_fsid
          when: ceph_conf_stat.stat.exists
          changed_when: false

        - name: Set fsid fact
          set_fact:
            fsid: "{{ discovered_fsid.stdout }}"
          when:
            - ceph_conf_stat.stat.exists
            - discovered_fsid.stdout != ""

    - name: Display purge information
      debug:
        msg: |
          =============================================
          WARNING: This will completely destroy the Ceph cluster!
          {% if fsid is defined %}
          FSID to purge: {{ fsid }}
          {% else %}
          Force purge mode - will remove all Ceph components
          {% endif %}
          =============================================

    - name: Purge using cephadm rm-cluster (if FSID available)
      when: fsid is defined
      block:
        - name: Check if cephadm is available
          command: which cephadm
          register: cephadm_path
          failed_when: false
          changed_when: false

        - name: Run cephadm rm-cluster
          command: cephadm rm-cluster --force --zap-osds --fsid {{ fsid }}
          when: cephadm_path.rc == 0
          ignore_errors: true

    - name: Stop all Ceph services
      block:
        - name: Stop Ceph systemd services
          systemd:
            name: "{{ item }}"
            state: stopped
          with_items:
            - ceph.target
            - ceph-mon.target
            - ceph-osd.target
            - ceph-mds.target
            - ceph-mgr.target
            - ceph-radosgw.target
          failed_when: false

        - name: Stop and disable all ceph-* services
          shell: |
            systemctl stop ceph-*.service ceph-*.target
            systemctl disable ceph-*.service ceph-*.target
          failed_when: false

    - name: Remove Podman/Docker containers
      block:
        - name: Check container runtime
          command: which {{ item }}
          register: container_runtime
          with_items:
            - podman
            - docker
          failed_when: false
          changed_when: false

        - name: Stop and remove all Ceph containers (Podman)
          shell: |
            podman stop $(podman ps -q --filter name=ceph) 2>/dev/null || true
            podman rm -f $(podman ps -aq --filter name=ceph) 2>/dev/null || true
          when: container_runtime.results[0].rc == 0
          failed_when: false

        - name: Stop and remove all Ceph containers (Docker)
          shell: |
            docker stop $(docker ps -q --filter name=ceph) 2>/dev/null || true
            docker rm -f $(docker ps -aq --filter name=ceph) 2>/dev/null || true
          when: container_runtime.results[1].rc == 0
          failed_when: false

    - name: Kill remaining Ceph processes
      shell: |
        pkill -9 -f ceph-mon || true
        pkill -9 -f ceph-osd || true
        pkill -9 -f ceph-mds || true
        pkill -9 -f ceph-mgr || true
        pkill -9 -f radosgw || true
        pkill -9 -f cephadm || true
      failed_when: false

    - name: Unmount Ceph filesystems
      shell: |
        umount -l /var/lib/ceph/osd/* 2>/dev/null || true
        umount -l /run/ceph/* 2>/dev/null || true
      failed_when: false

    - name: Remove Ceph LVM volumes
      shell: |
        # Remove all Ceph LVM volumes
        vgremove -f $(vgs | grep ceph | awk '{print $1}') 2>/dev/null || true
        pvremove -f $(pvs | grep ceph | awk '{print $1}') 2>/dev/null || true

        # Remove device mappings
        dmsetup ls | grep ceph | awk '{print $1}' | xargs -I {} dmsetup remove {} 2>/dev/null || true
      failed_when: false

    - name: Clean OSD disks
      shell: |
        # Find all OSD disks and zap them
        for dev in $(lsblk -no NAME,PARTLABEL | grep -i "ceph" | awk '{print "/dev/"$1}' | sed 's/[0-9]*$//' | sort -u); do
          if [ -b "$dev" ]; then
            echo "Zapping disk: $dev"
            wipefs -a "$dev" 2>/dev/null || true
            sgdisk --zap-all "$dev" 2>/dev/null || true
            dd if=/dev/zero of="$dev" bs=1M count=10 2>/dev/null || true
            partprobe "$dev" 2>/dev/null || true
          fi
        done
      failed_when: false

    - name: Remove Ceph directories
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /etc/ceph
        - /var/lib/ceph
        - /var/lib/cephadm
        - /var/run/ceph
        - /var/log/ceph
        - /tmp/ceph-*
      failed_when: false

    - name: Remove Ceph systemd units
      shell: |
        rm -rf /etc/systemd/system/ceph*.service
        rm -rf /etc/systemd/system/ceph*.target
        rm -rf /usr/lib/systemd/system/ceph*.service
        rm -rf /usr/lib/systemd/system/ceph*.target
        systemctl daemon-reload
      failed_when: false

    - name: Remove network interfaces (if any)
      shell: |
        ip link show | grep -E "ceph|br-ceph" | cut -d: -f2 | while read iface; do
          ip link delete $iface 2>/dev/null || true
        done
      failed_when: false

    - name: Clean up hosts file entries
      lineinfile:
        path: /etc/hosts
        regexp: '.*ceph.*'
        state: absent
      failed_when: false

    - name: Remove Ceph packages (optional - be careful!)
      when: remove_packages | default(false) | bool
      package:
        name:
          - cephadm
          - ceph-common
          - ceph-base
          - ceph-mon
          - ceph-osd
          - ceph-mds
          - ceph-mgr
          - ceph-radosgw
        state: absent
      failed_when: false

    - name: Final cleanup
      shell: |
        # Clean any remaining mounts
        mount | grep ceph | awk '{print $3}' | xargs -I {} umount -l {} 2>/dev/null || true

        # Clean shared memory
        rm -rf /dev/shm/ceph* 2>/dev/null || true

        # Reset failed systemd units
        systemctl reset-failed
      failed_when: false

- name: Verify purge completion
  hosts: all
  become: true
  gather_facts: false
  tasks:
    - name: Check for remaining Ceph processes
      shell: ps aux | grep -E 'ceph-|radosgw' | grep -v grep
      register: remaining_processes
      failed_when: false
      changed_when: false

    - name: Check for remaining containers
      shell: |
        (command -v podman >/dev/null 2>&1 && podman ps -a | grep ceph) || \
        (command -v docker >/dev/null 2>&1 && docker ps -a | grep ceph) || \
        echo "No containers found"
      register: remaining_containers
      failed_when: false
      changed_when: false

    - name: Report purge status
      debug:
        msg: |
          =============================================
          Purge Status for {{ inventory_hostname }}:
          - Remaining processes: {{ 'None' if remaining_processes.stdout == '' else 'FOUND - Manual cleanup needed!' }}
          - Remaining containers: {{ 'None' if 'No containers found' in remaining_containers.stdout or remaining_containers.stdout == '' else 'FOUND - Manual cleanup needed!' }}
          - Ceph directories removed: Yes
          =============================================

- name: Final summary
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Display completion message
      debug:
        msg: |
          =============================================
          CEPH CLUSTER PURGE COMPLETED!

          All Ceph components have been removed from all nodes.
          The systems are now ready for a fresh Ceph installation.

          Next steps:
          1. Verify all disks are clean: lsblk
          2. Reboot nodes if necessary: sudo reboot
          3. Run preflight playbook: ansible-playbook -i hosts-scalable.yaml cephadm-preflight.yml
          4. Bootstrap new cluster: sudo cephadm bootstrap --mon-ip <IP>
          =============================================