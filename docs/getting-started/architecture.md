# Ceph Architecture

Ceph ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ ìƒì„¸ ì•„í‚¤í…ì²˜ ê°€ì´ë“œ

## ğŸ“Š ì•„í‚¤í…ì²˜ ê°œìš”

```mermaid
graph TB
    subgraph "Client Layer"
        RBD_Client[RBD Client]
        CephFS_Client[CephFS Client]
        S3_Client[S3/Swift Client]
    end

    subgraph "Interface Layer"
        LIBRBD[librbd]
        LIBCEPHFS[libcephfs]
        RADOSGW[RGW]
    end

    subgraph "Core Storage Layer - RADOS"
        LIBRADOS[librados]
        MON[Monitor Cluster]
        MGR[Manager]
        OSD1[OSD.1]
        OSD2[OSD.2]
        OSD3[OSD.3]
        OSDN[OSD.N]
    end

    RBD_Client --> LIBRBD
    CephFS_Client --> LIBCEPHFS
    S3_Client --> RADOSGW

    LIBRBD --> LIBRADOS
    LIBCEPHFS --> LIBRADOS
    RADOSGW --> LIBRADOS

    LIBRADOS --> MON
    LIBRADOS --> OSD1
    LIBRADOS --> OSD2
    LIBRADOS --> OSD3
    LIBRADOS --> OSDN

    MON --> MGR
```

## ğŸ”§ ê³„ì¸µë³„ ìƒì„¸ ì•„í‚¤í…ì²˜

### 1. í´ë¼ì´ì–¸íŠ¸ ê³„ì¸µ (Client Layer)

í´ë¼ì´ì–¸íŠ¸ëŠ” Ceph í´ëŸ¬ìŠ¤í„°ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ë° ì‚¬ìš©ìì…ë‹ˆë‹¤.

#### í´ë¼ì´ì–¸íŠ¸ íƒ€ì…

- **Thick Clients**: libradosë¥¼ ì§ì ‘ ì‚¬ìš© (RBD, CephFS)
- **Thin Clients**: HTTP/S3 í”„ë¡œí† ì½œ ì‚¬ìš© (RGW)
- **Kernel Clients**: ì»¤ë„ ëª¨ë“ˆ í†µí•© (krbd, kcephfs)

#### í´ë¼ì´ì–¸íŠ¸ ì•„í‚¤í…ì²˜

```mermaid
graph LR
    subgraph "Application"
        App[Application]
    end

    subgraph "Client Libraries"
        App --> LibRBD[librbd]
        App --> LibCephFS[libcephfs]
        App --> S3SDK[S3 SDK]
    end

    subgraph "Protocol"
        LibRBD --> RADOS_Proto[RADOS Protocol]
        LibCephFS --> RADOS_Proto
        S3SDK --> HTTP[HTTP/S3]
    end

    RADOS_Proto --> Cluster[Ceph Cluster]
    HTTP --> RGW[RGW] --> Cluster
```

### 2. RADOS ê³„ì¸µ (Core Storage)

RADOSëŠ” Cephì˜ í•µì‹¬ ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

#### RADOS êµ¬ì„± ìš”ì†Œ

```mermaid
graph TB
    subgraph "RADOS Cluster"
        subgraph "Monitor Quorum"
            MON1[MON.1<br/>Leader]
            MON2[MON.2<br/>Peon]
            MON3[MON.3<br/>Peon]
            MON1 -.->|Paxos| MON2
            MON2 -.->|Paxos| MON3
            MON3 -.->|Paxos| MON1
        end

        subgraph "Manager Active/Standby"
            MGR1[MGR.1<br/>Active]
            MGR2[MGR.2<br/>Standby]
        end

        subgraph "OSD Cluster"
            OSD1[OSD.1]
            OSD2[OSD.2]
            OSD3[OSD.3]
            OSD4[OSD.4]
            OSD5[OSD.5]
            OSD6[OSD.6]
        end

        MON1 --> MGR1
        MON1 --> OSD1
        MON1 --> OSD2
        MON1 --> OSD3
    end
```

### 3. ë°ì´í„° í”Œë¡œìš° ì•„í‚¤í…ì²˜

#### ì“°ê¸° ì‘ì—… í”Œë¡œìš°

```mermaid
sequenceDiagram
    participant Client
    participant MON
    participant Primary_OSD
    participant Replica_OSD1
    participant Replica_OSD2

    Client->>MON: 1. Get Cluster Map
    MON->>Client: 2. Return Map + Auth

    Note over Client: 3. CRUSH ê³„ì‚°

    Client->>Primary_OSD: 4. Write Request
    Primary_OSD->>Replica_OSD1: 5. Replicate
    Primary_OSD->>Replica_OSD2: 5. Replicate

    Replica_OSD1->>Primary_OSD: 6. ACK
    Replica_OSD2->>Primary_OSD: 6. ACK

    Primary_OSD->>Client: 7. Write Complete
```

#### ì½ê¸° ì‘ì—… í”Œë¡œìš°

```mermaid
sequenceDiagram
    participant Client
    participant MON
    participant Primary_OSD

    Client->>MON: 1. Get Cluster Map (ìºì‹œ í™•ì¸)
    MON->>Client: 2. Return Map (í•„ìš”ì‹œ)

    Note over Client: 3. CRUSH ê³„ì‚°

    Client->>Primary_OSD: 4. Read Request
    Primary_OSD->>Client: 5. Return Data
```

## ğŸ—ºï¸ CRUSH ë§µ êµ¬ì¡°

### CRUSH ê³„ì¸µ êµ¬ì¡°

```mermaid
graph TD
    Root[Root: default]
    Root --> DC1[Datacenter: dc1]
    Root --> DC2[Datacenter: dc2]

    DC1 --> Rack1[Rack: rack1]
    DC1 --> Rack2[Rack: rack2]

    Rack1 --> Host1[Host: ceph1]
    Rack1 --> Host2[Host: ceph2]

    Rack2 --> Host3[Host: ceph3]
    Rack2 --> Host4[Host: ceph4]

    Host1 --> OSD1[OSD.1]
    Host1 --> OSD2[OSD.2]

    Host2 --> OSD3[OSD.3]
    Host2 --> OSD4[OSD.4]

    Host3 --> OSD5[OSD.5]
    Host3 --> OSD6[OSD.6]

    Host4 --> OSD7[OSD.7]
    Host4 --> OSD8[OSD.8]
```

### CRUSH ì•Œê³ ë¦¬ì¦˜ í”Œë¡œìš°

```mermaid
graph LR
    Object[Object Name] --> Hash[Hash Function]
    Hash --> PG_ID[PG ID]
    PG_ID --> CRUSH[CRUSH Algorithm]

    subgraph "CRUSH Calculation"
        CRUSH --> Rule[CRUSH Rule]
        Rule --> Map[CRUSH Map]
        Map --> OSDs[OSD Set]
    end

    OSDs --> Primary[Primary OSD]
    OSDs --> Replicas[Replica OSDs]
```

## ğŸ“ ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜

### RBD (Block Storage)

```mermaid
graph TB
    subgraph "RBD Architecture"
        VM[Virtual Machine]
        VM --> QEMU[QEMU/KVM]
        QEMU --> LIBRBD[librbd]

        Container[Container]
        Container --> CSI[Ceph CSI]
        CSI --> LIBRBD

        LIBRBD --> Objects[4MB Objects]
        Objects --> Pool[RBD Pool]
        Pool --> OSDs[OSD Cluster]
    end
```

**RBD íŠ¹ì§•:**
- ë¸”ë¡ ë””ë°”ì´ìŠ¤ ì¶”ìƒí™”
- ì”¬ í”„ë¡œë¹„ì €ë‹
- ìŠ¤ëƒ…ìƒ· ë° í´ë¡ 
- ì´ë¯¸ì§€ ë ˆì´ì–´ë§

### CephFS (File System)

```mermaid
graph TB
    subgraph "CephFS Architecture"
        Client[CephFS Client]
        Client --> MDS_Active[MDS Active]
        Client --> MDS_Standby[MDS Standby]

        MDS_Active --> Meta_Pool[Metadata Pool]
        MDS_Active --> Data_Pool[Data Pool]

        Meta_Pool --> OSD_Meta[Metadata OSDs]
        Data_Pool --> OSD_Data[Data OSDs]

        MDS_Active -.->|Failover| MDS_Standby
    end
```

**CephFS êµ¬ì„± ìš”ì†Œ:**
- **MDS**: ë©”íƒ€ë°ì´í„° ì„œë²„
- **Metadata Pool**: íŒŒì¼ ì‹œìŠ¤í…œ ë©”íƒ€ë°ì´í„°
- **Data Pool**: ì‹¤ì œ íŒŒì¼ ë°ì´í„°

### RGW (Object Storage)

```mermaid
graph TB
    subgraph "RGW Architecture"
        S3Client[S3 Client]
        SwiftClient[Swift Client]

        S3Client --> RGW1[RGW Instance 1]
        SwiftClient --> RGW2[RGW Instance 2]

        RGW1 --> Index[Index Pool]
        RGW1 --> Data[Data Pool]
        RGW2 --> Index
        RGW2 --> Data

        subgraph "Pools"
            Index --> OSD_Index[Index OSDs]
            Data --> OSD_Data[Data OSDs]
        end

        RGW1 -.->|Load Balance| RGW2
    end
```

## ğŸ”„ Placement Group (PG) ì•„í‚¤í…ì²˜

### PG ë§¤í•‘ êµ¬ì¡°

```mermaid
graph TD
    subgraph "Object to PG Mapping"
        Obj1[Object 1] --> PG1[PG 1.0]
        Obj2[Object 2] --> PG1
        Obj3[Object 3] --> PG2[PG 1.1]
        Obj4[Object 4] --> PG2
    end

    subgraph "PG to OSD Mapping"
        PG1 --> OSD_Set1[OSD Set: 1,3,5]
        PG2 --> OSD_Set2[OSD Set: 2,4,6]
    end

    subgraph "Physical OSDs"
        OSD_Set1 --> OSD1[OSD.1<br/>Primary]
        OSD_Set1 --> OSD3[OSD.3<br/>Replica]
        OSD_Set1 --> OSD5[OSD.5<br/>Replica]

        OSD_Set2 --> OSD2[OSD.2<br/>Primary]
        OSD_Set2 --> OSD4[OSD.4<br/>Replica]
        OSD_Set2 --> OSD6[OSD.6<br/>Replica]
    end
```

### PG ìƒíƒœ ë¨¸ì‹ 

```mermaid
stateDiagram-v2
    [*] --> Creating
    Creating --> Active

    Active --> Active_Clean: All replicas in sync
    Active --> Active_Recovery: Missing objects
    Active --> Active_Backfilling: Adding new OSD

    Active_Recovery --> Active_Clean: Recovery complete
    Active_Backfilling --> Active_Clean: Backfill complete

    Active_Clean --> Active_Scrubbing: Scheduled scrub
    Active_Scrubbing --> Active_Clean: Scrub complete

    Active --> Peering: OSD failure
    Peering --> Active: Peering complete

    Active --> Inactive: Not enough OSDs
    Inactive --> Active: OSDs available
```

## ğŸ” ì¸ì¦ ë° ë³´ì•ˆ ì•„í‚¤í…ì²˜

### CephX ì¸ì¦ í”Œë¡œìš°

```mermaid
sequenceDiagram
    participant Client
    participant Monitor
    participant OSD

    Client->>Monitor: 1. Auth Request + Username
    Monitor->>Monitor: 2. Verify Credentials
    Monitor->>Client: 3. Session Key + Ticket

    Client->>OSD: 4. Request + Ticket
    OSD->>OSD: 5. Verify Ticket
    OSD->>Client: 6. Authorized Access

    Note over Client,OSD: Tickets have TTL and capabilities
```

### ë³´ì•ˆ ê³„ì¸µ

```mermaid
graph TB
    subgraph "Security Layers"
        Network[Network Encryption<br/>In-transit]
        Auth[CephX Authentication]
        Caps[Capabilities<br/>Authorization]
        Encryption[At-rest Encryption<br/>OSD/RGW]
    end

    Network --> Auth
    Auth --> Caps
    Caps --> Encryption
```

## ğŸš€ ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜

### Monitor Quorum

```mermaid
graph TB
    subgraph "Monitor HA"
        direction LR
        MON1[Monitor 1<br/>Leader]
        MON2[Monitor 2<br/>Peon]
        MON3[Monitor 3<br/>Peon]
        MON4[Monitor 4<br/>Peon]
        MON5[Monitor 5<br/>Peon]

        MON1 -.->|Paxos Consensus| MON2
        MON1 -.->|Paxos Consensus| MON3
        MON1 -.->|Paxos Consensus| MON4
        MON1 -.->|Paxos Consensus| MON5
    end

    Note1[Quorum = 3 for 5 monitors]
```

### ì¥ì•  ë„ë©”ì¸

```mermaid
graph TD
    subgraph "Failure Domains"
        subgraph "Rack 1"
            Host1[Host 1]
            Host2[Host 2]
        end

        subgraph "Rack 2"
            Host3[Host 3]
            Host4[Host 4]
        end

        subgraph "Rack 3"
            Host5[Host 5]
            Host6[Host 6]
        end

        Object[Object: 3 Replicas]
        Object --> Host1
        Object --> Host3
        Object --> Host5

        Note1[ê° ë³µì œë³¸ì€ ë‹¤ë¥¸ Rackì— ë°°ì¹˜]
    end
```

## ğŸ“ˆ í™•ì¥ì„± ì•„í‚¤í…ì²˜

### ìˆ˜í‰ í™•ì¥

```mermaid
graph LR
    subgraph "Initial Cluster"
        Init_OSD[3 Nodes<br/>12 OSDs]
    end

    subgraph "Scaled Cluster"
        Scale_OSD[10 Nodes<br/>40 OSDs]
    end

    subgraph "Large Cluster"
        Large_OSD[100 Nodes<br/>400 OSDs]
    end

    Init_OSD -->|Add Nodes| Scale_OSD
    Scale_OSD -->|Add More| Large_OSD

    Note1[CRUSHê°€ ìë™ìœ¼ë¡œ ë°ì´í„° ì¬ë¶„ë°°]
```

### ì„±ëŠ¥ í™•ì¥

```mermaid
graph TB
    subgraph "Performance Scaling"
        subgraph "Cache Tier"
            SSD_Pool[SSD Pool<br/>Hot Data]
        end

        subgraph "Storage Tier"
            HDD_Pool[HDD Pool<br/>Cold Data]
        end

        Client[Client] --> SSD_Pool
        SSD_Pool -->|Tier Agent| HDD_Pool

        Note1[ìë™ í‹°ì–´ë§ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”]
    end
```

## ğŸ”§ BlueStore ì•„í‚¤í…ì²˜

### BlueStore ìŠ¤íƒ

```mermaid
graph TB
    subgraph "BlueStore Stack"
        Client[Ceph Client]
        Client --> OSD_Layer[OSD]
        OSD_Layer --> BlueStore[BlueStore]

        subgraph "BlueStore Components"
            BlueStore --> RocksDB[RocksDB<br/>Metadata]
            BlueStore --> BlockDevice[Block Device<br/>Direct I/O]
            BlueStore --> BlueFS[BlueFS]

            RocksDB --> BlueFS
            BlueFS --> Device[Physical Device]
            BlockDevice --> Device
        end
    end
```

### BlueStore íŠ¹ì§•

- **ì§ì ‘ ë¸”ë¡ ì ‘ê·¼**: íŒŒì¼ ì‹œìŠ¤í…œ ì˜¤ë²„í—¤ë“œ ì œê±°
- **íš¨ìœ¨ì  ë©”íƒ€ë°ì´í„°**: RocksDB ì‚¬ìš©
- **ì²´í¬ì„¬**: ë°ì´í„° ë¬´ê²°ì„±
- **ì••ì¶•**: ì¸ë¼ì¸ ì••ì¶• ì§€ì›
- **íš¨ìœ¨ì  ì˜¤ë²„ë¼ì´íŠ¸**: Copy-on-Write ìµœì†Œí™”

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ì•„í‚¤í…ì²˜

### ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ

```mermaid
graph TB
    subgraph "Monitoring Stack"
        MGR[Ceph Manager]
        MGR --> Prometheus[Prometheus Module]
        MGR --> Dashboard[Dashboard Module]
        MGR --> Zabbix[Zabbix Module]

        Prometheus --> Grafana[Grafana]
        Dashboard --> WebUI[Web Dashboard]
        Zabbix --> ZabbixServer[Zabbix Server]

        MGR --> Telegraf[Telegraf Module]
        Telegraf --> InfluxDB[InfluxDB]
    end
```

## ğŸŒ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜

### ë“€ì–¼ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

```mermaid
graph TB
    subgraph "Network Architecture"
        subgraph "Public Network"
            Client[Clients]
            MON[Monitors]
            MGR[Managers]
            RGW[RGW]
        end

        subgraph "Cluster Network"
            OSD1[OSD 1]
            OSD2[OSD 2]
            OSD3[OSD 3]
            OSDN[OSD N]
        end

        Client -.->|Client Traffic| OSD1
        MON -.->|Monitor Traffic| OSD1

        OSD1 <-->|Replication| OSD2
        OSD2 <-->|Recovery| OSD3
        OSD3 <-->|Backfill| OSDN
    end

    Note1[Public: 10.10.2.0/24]
    Note2[Cluster: 192.168.2.0/24]
```

## ìš”ì•½

Ceph ì•„í‚¤í…ì²˜ëŠ” RADOSë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë¶„ì‚° ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œìœ¼ë¡œ, CRUSH ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ë°ì´í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì‚°í•˜ê³ , ë‹¤ì–‘í•œ ìŠ¤í† ë¦¬ì§€ ì¸í„°í˜ì´ìŠ¤(RBD, CephFS, RGW)ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Monitor ì¿¼ëŸ¼ì€ í´ëŸ¬ìŠ¤í„° ìƒíƒœë¥¼ ê´€ë¦¬í•˜ê³ , OSDëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ë©°, ManagerëŠ” ëª¨ë‹ˆí„°ë§ê³¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ì´ ëª¨ë“  êµ¬ì„± ìš”ì†Œê°€ í˜‘ë ¥í•˜ì—¬ í™•ì¥ ê°€ëŠ¥í•˜ê³  ìê°€ ì¹˜ìœ ê°€ ê°€ëŠ¥í•œ ìŠ¤í† ë¦¬ì§€ í”Œë«í¼ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

---

*ì´ ë¬¸ì„œëŠ” Ceph ê³µì‹ ë¬¸ì„œ(docs.ceph.com) ë° Red Hat Ceph Storage ì•„í‚¤í…ì²˜ ê°€ì´ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*